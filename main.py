import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from typing import List, Dict, Optional
import os
import re

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api.message_components import Plain
from astrbot.api import logger
from astrbot.core.config import AstrBotConfig


@register(
    "astrbot_plugin_soushuba",
    "Foolllll",
    "æœä¹¦å§é“¾æ¥æå–å™¨",
    "1.0.0",
    "https://github.com/Foolllll-J/astrbot_plugin_soushuba",
)
class SoushuBaLinkExtractorPlugin(Star):
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        self.target_domains: List[str] = [
            "https://soushu2022.com",
            "https://soushu2025.com",
            "https://soushu2030.com",
            "https://soushu2035.com",
        ]
        self.plugin_config = config


    # ä¿®æ”¹æ­¤å¤„ï¼šæ·»åŠ ä¸€ä¸ªå¯é€‰å‚æ•° start_urlï¼Œé»˜è®¤ä¸º None
    async def _extract_link_from_url(self, session: aiohttp.ClientSession, url: str, start_url: Optional[str] = None) -> str:
        """å°è¯•è®¿é—®URLå¹¶æå–æŒ‡å®šé“¾æ¥ï¼Œå³ä½¿çŠ¶æ€ç æ˜¯404ä¹Ÿä¼šå°è¯•è§£æå†…å®¹ã€‚
        start_url æ˜¯æœ€åˆçš„è¯·æ±‚URLï¼Œç”¨äºæœ€ç»ˆæ¶ˆæ¯æŠ¥å‘Šã€‚
        """
        # å¦‚æœæ˜¯é¦–æ¬¡è°ƒç”¨ï¼Œå°†å½“å‰URLä½œä¸ºstart_url
        if start_url is None:
            start_url = url

        try:
            ssl_verify = False if url.startswith("https://") else True
            if not ssl_verify:
                logger.warning(f"ç”±äºè¯ä¹¦é—®é¢˜ï¼Œè®¿é—® {url} å°†ç¦ç”¨ SSL éªŒè¯ã€‚")
            
            async with session.get(url, timeout=20, ssl=ssl_verify) as response:
                final_url = str(response.url)
                html_content = await response.text()
                status_code = response.status
            
            logger.info(f"æˆåŠŸè®¿é—® {url}ã€‚æœ€ç»ˆURL: {final_url}ï¼ŒçŠ¶æ€ç : {status_code}ã€‚HTMLå†…å®¹é•¿åº¦: {len(html_content)} å­—èŠ‚ã€‚")
            logger.debug(f"HTML Content preview (first 500 chars from {final_url}): \n{html_content[:500]}...") 

            # æ­¥éª¤1: æ£€æŸ¥å¹¶å¤„ç† JavaScript é‡å®šå‘
            js_redirect_match = re.search(r"window\.location\.href\s*=\s*['\"](.*?)['\"];", html_content)
            if js_redirect_match:
                redirect_target_url = urljoin(final_url, js_redirect_match.group(1))
                logger.info(f"æ£€æµ‹åˆ° JavaScript è·³è½¬åˆ°: {redirect_target_url}ï¼Œå†æ¬¡è¯·æ±‚ã€‚")
                # **ä¿®æ”¹æ­¤å¤„ï¼šé€’å½’è°ƒç”¨æ—¶ä¼ å…¥åŸå§‹çš„ start_url**
                return await self._extract_link_from_url(session, redirect_target_url, start_url)

            # æ­¥éª¤2: æ£€æŸ¥å¹¶å¤„ç† Meta Refresh é‡å®šå‘
            meta_refresh_match = re.search(r"<meta http-equiv=\"refresh\" content=\"[\d\.]*;\s*url=(.*?)\"", html_content, re.IGNORECASE)
            if meta_refresh_match:
                redirect_target_url = urljoin(final_url, meta_refresh_match.group(1))
                logger.info(f"æ£€æµ‹åˆ° Meta Refresh è·³è½¬åˆ°: {redirect_target_url}ï¼Œå†æ¬¡è¯·æ±‚ã€‚")
                # **ä¿®æ”¹æ­¤å¤„ï¼šé€’å½’è°ƒç”¨æ—¶ä¼ å…¥åŸå§‹çš„ start_url**
                return await self._extract_link_from_url(session, redirect_target_url, start_url)

            # æ­¥éª¤3: æ‰§è¡Œ BeautifulSoup æŸ¥æ‰¾
            soup = BeautifulSoup(html_content, 'lxml') 
            link_element = None

            link_element = soup.select_one('a.link') 
            if not link_element:
                link_element = soup.find('a', string='æœä¹¦å§')
            if not link_element:
                link_element = soup.find('a')

            if link_element and link_element.has_attr('href'):
                link_url = link_element['href']
                if not link_url.startswith(('http://', 'https://')):
                    link_url = urljoin(final_url, link_url)
                
                logger.info(f"BeautifulSoup æœ€ç»ˆæ‰¾åˆ°é“¾æ¥: {link_url}")
                # **ä¿®æ”¹æ­¤å¤„ï¼šåœ¨è¿”å›æˆåŠŸæ¶ˆæ¯æ—¶ä½¿ç”¨ start_url**
                return f"âœ… æˆåŠŸæ‰¾åˆ°é“¾æ¥äº {start_url}:\n{link_url}"
            else:
                logger.warning(f"æ‰€æœ‰æŸ¥æ‰¾ç­–ç•¥å‡æœªèƒ½åœ¨ {final_url} æ‰¾åˆ°æœ‰æ•ˆé“¾æ¥ã€‚")
                # **ä¿®æ”¹æ­¤å¤„ï¼šåœ¨è¿”å›ä¿¡æ¯æ€§æ¶ˆæ¯æ—¶ä½¿ç”¨ start_url**
                return f"â„¹ï¸ è®¿é—® {start_url} æˆåŠŸï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„é“¾æ¥å…ƒç´ ã€‚"

        except aiohttp.ClientError as e: 
            logger.error(f"âŒ è®¿é—® {url} å¤±è´¥: ç½‘ç»œè¿æ¥é”™è¯¯ - {e}")
            # **ä¿®æ”¹æ­¤å¤„ï¼šåœ¨è¿”å›é”™è¯¯æ¶ˆæ¯æ—¶ä½¿ç”¨ start_url**
            return f"âŒ è®¿é—® {start_url} å¤±è´¥: ç½‘ç»œè¿æ¥é”™è¯¯ - {e}"
        except asyncio.TimeoutError: 
            logger.error(f"âŒ è®¿é—® {url} è¶…æ—¶ã€‚")
            # **ä¿®æ”¹æ­¤å¤„ï¼šåœ¨è¿”å›è¶…æ—¶é”™è¯¯æ—¶ä½¿ç”¨ start_url**
            return f"âŒ è®¿é—® {start_url} è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        except Exception as e: 
            logger.error(f"âŒ è®¿é—® {url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            # **ä¿®æ”¹æ­¤å¤„ï¼šåœ¨è¿”å›æœªçŸ¥é”™è¯¯æ—¶ä½¿ç”¨ start_url**
            return f"âŒ è®¿é—® {start_url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

    @filter.command("ssb")
    async def ssb_command(self, event: AstrMessageEvent):
        """
        ä¾æ¬¡å°è¯•è®¿é—®é¢„è®¾åˆ—è¡¨ä¸­çš„æœä¹¦ç½‘ç«™ï¼Œå¹¶è¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸè®¿é—®åˆ°çš„é¡µé¢ä¸­çš„ç¬¬ä¸€ä¸ªé“¾æ¥ã€‚
        ç”¨æ³•: /ssb
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /ssb å‘½ä»¤ï¼Œå¼€å§‹æœä¹¦ã€‚")
        yield event.plain_result("ğŸš€ æ­£åœ¨å°è¯•è®¿é—®æœä¹¦ç½‘ç«™ï¼Œè¯·ç¨å€™...")
        
        async with aiohttp.ClientSession() as session:
            for domain_url in self.target_domains:
                logger.info(f"æ­£åœ¨å°è¯•è®¿é—®: {domain_url}")
                result_message = await self._extract_link_from_url(session, domain_url, domain_url) 
                
                if result_message.startswith("âœ…") or result_message.startswith("â„¹ï¸"):
                    yield event.plain_result(result_message)
                    return
                else:
                    logger.warning(f"è®¿é—® {domain_url} å¤±è´¥ï¼Œæ­£åœ¨å°è¯•ä¸‹ä¸€ä¸ª...")
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæ‰€æœ‰é¢„è®¾ç½‘ç«™å‡æ— æ³•è®¿é—®æˆ–æœªæ‰¾åˆ°å¯ç”¨é“¾æ¥ã€‚")


    async def terminate(self):
        """æ’ä»¶é”€æ¯æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("æœä¹¦å§é“¾æ¥æå–å™¨æ’ä»¶å·²å¸è½½")