import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
import os
import re

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api.message_components import Plain
from astrbot.api import logger
from astrbot.core.config import AstrBotConfig

DEFAULT_SXSY_HOST = "sxsy19.com" # é»˜è®¤åŸŸå

@register(
    "astrbot_plugin_soushuba",
    "Foolllll",
    "æœä¹¦å§é“¾æ¥è·å–",
    "1.1.0",
    "https://github.com/Foolllll-J/astrbot_plugin_soushuba",
)
class SoushuBaLinkExtractorPlugin(Star):
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        self.target_domains: List[str] = [
            "https://soushu2022.com",
            "https://soushu2025.com",
            "https://soushu2030.com",
            "https://soushu2035.com",
        ]
        self.plugin_config = config


    async def _extract_link_from_url(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """å°è¯•è®¿é—®URLå¹¶æå–æŒ‡å®šé“¾æ¥ã€‚æˆåŠŸåˆ™è¿”å›é“¾æ¥ï¼Œå¤±è´¥è¿”å› Noneã€‚"""
        try:
            ssl_verify = False if url.startswith("https://") else True
            
            async with session.get(url, timeout=20, ssl=ssl_verify) as response:
                final_url = str(response.url)
                html_content = await response.text()
            
            # æ­¥éª¤1: æ£€æŸ¥å¹¶å¤„ç† JavaScript é‡å®šå‘
            js_redirect_match = re.search(r"window\.location\.href\s*=\s*['\"](.*?)['\"];", html_content)
            if js_redirect_match:
                redirect_target_url = urljoin(final_url, js_redirect_match.group(1))
                return await self._extract_link_from_url(session, redirect_target_url)

            # æ­¥éª¤2: æ£€æŸ¥å¹¶å¤„ç† Meta Refresh é‡å®šå‘
            meta_refresh_match = re.search(r"<meta http-equiv=\"refresh\" content=\"[\d\.]*;\s*url=(.*?)\"", html_content, re.IGNORECASE)
            if meta_refresh_match:
                redirect_target_url = urljoin(final_url, meta_refresh_match.group(1))
                return await self._extract_link_from_url(session, redirect_target_url)

            # æ­¥éª¤3: æ‰§è¡Œ BeautifulSoup æŸ¥æ‰¾
            soup = BeautifulSoup(html_content, 'lxml') 
            link_element = soup.select_one('a.link') 
            if not link_element:
                link_element = soup.find('a', string='æœä¹¦å§')
            if not link_element:
                link_element = soup.find('a')

            if link_element and link_element.has_attr('href'):
                link_url = link_element['href']
                if not link_url.startswith(('http://', 'https://')):
                    link_url = urljoin(final_url, link_url)
                return link_url

        except Exception as e: 
            logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
        return None

    @filter.command("ssb", alias={'æœä¹¦å§'})
    async def ssb_command(self, event: AstrMessageEvent):
        """
        è·å–æœä¹¦å§çš„ç½‘å€ã€‚
        ç”¨æ³•: /ssb
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /ssb å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾æœä¹¦å§ç½‘å€ã€‚")
        
        async with aiohttp.ClientSession() as session:
            for domain_url in self.target_domains:
                link_url = await self._extract_link_from_url(session, domain_url)
                if link_url:
                    yield event.plain_result(f"ğŸ“– æˆåŠŸæ‰¾åˆ°æœä¹¦å§æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                    return
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæ‰€æœ‰å¯¼èˆªç½‘ç«™å‡æ— æ³•è®¿é—®æˆ–æœªæ‰¾åˆ°å¯ç”¨é“¾æ¥ã€‚")

    @filter.command("sxsy", alias={'å°šé¦™ä¹¦è‹‘'})
    async def sxsy_command(self, event: AstrMessageEvent):
        """
        è·å–å°šé¦™ä¹¦è‹‘çš„ç½‘å€ã€‚
        ç”¨æ³•: /sxsy
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /sxsy å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾å°šé¦™ä¹¦è‹‘ç½‘å€ã€‚")

        async with aiohttp.ClientSession() as session:
            try:
                url = "https://sxsy.org/"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                }
                async with session.get(url, headers=headers, timeout=10, ssl=False) as response:
                    if response.status == 200:
                        text = await response.text()
                        match = re.search(r'href="https://([^"]+)"', text)
                        if match:
                            host = match.group(1)
                            yield event.plain_result(f"ğŸŒ¸ æˆåŠŸæ‰¾åˆ°å°šé¦™ä¹¦è‹‘æœ€æ–°ç½‘å€ï¼š\nhttps://{host}")
                            return
                    
                    yield event.plain_result(f"ğŸŒ¸ å°šé¦™ä¹¦è‹‘æœ€æ–°ç½‘å€ï¼š\nhttps://{DEFAULT_SXSY_HOST}")

            except Exception as e:
                logger.error(f"[è·å–sxsy host] å‘ç”Ÿé”™è¯¯: {e}")
                yield event.plain_result(f"ğŸŒ¸ å°šé¦™ä¹¦è‹‘ç›®å‰ç½‘å€ï¼š\nhttps://{DEFAULT_SXSY_HOST}")

    @filter.command("sis", alias={'ç¬¬ä¸€ä¼šæ‰€'})
    async def sis_command(self, event: AstrMessageEvent):
        """
        è·å–ç¬¬ä¸€ä¼šæ‰€çš„ç½‘å€ã€‚
        ç”¨æ³•: /sis
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /sis å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾ç¬¬ä¸€ä¼šæ‰€ç½‘å€ã€‚")
        
        target_navs = ["http://sis001dz.org/", "http://www.sis001home.com/"]
        
        async with aiohttp.ClientSession() as session:
            for url in target_navs:
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                    }
                    async with session.get(url, headers=headers, timeout=10) as response:
                        if response.status == 200:
                            text = await response.text()
                            soup = BeautifulSoup(text, 'lxml')
                            # æŸ¥æ‰¾åŒ…å«â€œåœ°å€ä¸€â€æ–‡æœ¬çš„ <a> æ ‡ç­¾
                            link_element = soup.find('a', string=re.compile(r'åœ°å€ä¸€'))
                            if link_element and link_element.has_attr('href'):
                                link_url = link_element['href']
                                yield event.plain_result(f"ğŸ” æˆåŠŸæ‰¾åˆ°ç¬¬ä¸€ä¼šæ‰€æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                                return
                except Exception as e:
                    logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
                    continue
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œç¬¬ä¸€ä¼šæ‰€å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    async def terminate(self):
        """æ’ä»¶é”€æ¯æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("æœä¹¦å§é“¾æ¥è·å–æ’ä»¶å·²å¸è½½")