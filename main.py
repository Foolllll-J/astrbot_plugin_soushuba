import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
import os
import re

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api.message_components import Plain
from astrbot.api import logger

@register(
    "astrbot_plugin_soushuba",
    "Foolllll",
    "æœä¹¦å§é“¾æ¥è·å–",
    "1.1.0",
    "https://github.com/Foolllll-J/astrbot_plugin_soushuba",
)
class SoushuBaLinkExtractorPlugin(Star):
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        self.target_domains: List[str] = [
            "https://soushu2022.com",
            "https://soushu2025.com",
            "https://soushu2030.com",
            "https://soushu2035.com",
        ]
        self.plugin_config = config

    async def _get_text(self, response: aiohttp.ClientResponse) -> str:
        """è·å–å“åº”å†…å®¹å¹¶å¤„ç†ç¼–ç é—®é¢˜"""
        content = await response.read()
        
        # å°è¯•ä» Content-Type è·å–ç¼–ç 
        charset = response.charset
        if charset:
            try:
                return content.decode(charset)
            except:
                pass
        
        # å°è¯•å¸¸è§çš„ä¸­æ–‡ç¼–ç 
        for encoding in ['utf-8', 'gbk', 'gb2312', 'big5']:
            try:
                return content.decode(encoding)
            except UnicodeDecodeError:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨ ignore æ¨¡å¼è§£ç 
        return content.decode('utf-8', errors='ignore')

    async def _extract_link_from_url(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """å°è¯•è®¿é—®URLå¹¶æå–æŒ‡å®šé“¾æ¥ã€‚æˆåŠŸåˆ™è¿”å›é“¾æ¥ï¼Œå¤±è´¥è¿”å› Noneã€‚"""
        try:
            ssl_verify = False if url.startswith("https://") else True
            
            async with session.get(url, timeout=20, ssl=ssl_verify) as response:
                final_url = str(response.url)
                html_content = await self._get_text(response)
            
            # æ­¥éª¤1: æ£€æŸ¥å¹¶å¤„ç† JavaScript é‡å®šå‘
            js_redirect_match = re.search(r"window\.location\.href\s*=\s*['\"](.*?)['\"];", html_content)
            if js_redirect_match:
                redirect_target_url = urljoin(final_url, js_redirect_match.group(1))
                return await self._extract_link_from_url(session, redirect_target_url)

            # æ­¥éª¤2: æ£€æŸ¥å¹¶å¤„ç† Meta Refresh é‡å®šå‘
            meta_refresh_match = re.search(r"<meta http-equiv=\"refresh\" content=\"[\d\.]*;\s*url=(.*?)\"", html_content, re.IGNORECASE)
            if meta_refresh_match:
                redirect_target_url = urljoin(final_url, meta_refresh_match.group(1))
                return await self._extract_link_from_url(session, redirect_target_url)

            # æ­¥éª¤3: æ‰§è¡Œ BeautifulSoup æŸ¥æ‰¾
            soup = BeautifulSoup(html_content, 'lxml') 
            link_element = soup.select_one('a.link') 
            if not link_element:
                link_element = soup.find('a', string='æœä¹¦å§')
            if not link_element:
                link_element = soup.find('a')

            if link_element and link_element.has_attr('href'):
                link_url = link_element['href']
                if not link_url.startswith(('http://', 'https://')):
                    link_url = urljoin(final_url, link_url)
                return link_url

        except Exception as e: 
            logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
        return None

    @filter.command("ssb", alias={'æœä¹¦å§'})
    async def ssb_command(self, event: AstrMessageEvent):
        """
        è·å–æœä¹¦å§çš„ç½‘å€ã€‚
        ç”¨æ³•: /ssb
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /ssb å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾æœä¹¦å§ç½‘å€ã€‚")
        
        async with aiohttp.ClientSession() as session:
            for domain_url in self.target_domains:
                link_url = await self._extract_link_from_url(session, domain_url)
                if link_url:
                    yield event.plain_result(f"ğŸ“– æˆåŠŸæ‰¾åˆ°æœä¹¦å§æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                    return
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæ‰€æœ‰å¯¼èˆªç½‘ç«™å‡æ— æ³•è®¿é—®æˆ–æœªæ‰¾åˆ°å¯ç”¨é“¾æ¥ã€‚")

    @filter.command("sxsy", alias={'å°šé¦™ä¹¦è‹‘'})
    async def sxsy_command(self, event: AstrMessageEvent):
        """
        å°šé¦™ä¹¦è‹‘æœç´¢ã€‚
        ç”¨æ³•: /sxsy <å…³é”®è¯>
        """
        args = event.message_str.strip().split(maxsplit=1)
        if len(args) < 2:
            # åŸæœ‰çš„è·å–ç½‘å€é€»è¾‘
            logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /sxsy å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾å°šé¦™ä¹¦è‹‘ç½‘å€ã€‚")
            async with aiohttp.ClientSession() as session:
                try:
                    url = "https://sxsy.org/"
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                    }
                    async with session.get(url, headers=headers, timeout=10, ssl=False) as response:
                        if response.status == 200:
                            text = await self._get_text(response)
                            match = re.search(r'href="https://([^"]+)"', text)
                            if match:
                                host = match.group(1)
                                yield event.plain_result(f"ğŸŒ¸ æˆåŠŸæ‰¾åˆ°å°šé¦™ä¹¦è‹‘æœ€æ–°ç½‘å€ï¼š\nhttps://{host}")
                                return
                except Exception as e:
                    logger.error(f"[è·å–sxsy host] å‘ç”Ÿé”™è¯¯: {e}")
            yield event.plain_result("âŒ æŠ±æ­‰ï¼Œå°šé¦™ä¹¦è‹‘å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")
            return

        keyword = args[1]
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /sxsy æœç´¢: {keyword}")
        
        cookie = self.plugin_config.get("sxsy_cookie", "") if self.plugin_config else ""
        if not cookie:
            yield event.plain_result("âŒ è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­è®¾ç½® sxsy_cookieã€‚")
            return

        yield event.plain_result(f"ğŸ” æ­£åœ¨å°šé¦™ä¹¦è‹‘æœç´¢: {keyword}...")

        async with aiohttp.ClientSession() as session:
            try:
                # 1. è·å–æœ€æ–° host
                host = "sxsy87.com" # é»˜è®¤å€¼
                try:
                    async with session.get("https://sxsy.org/", timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            t = await self._get_text(resp)
                            m = re.search(r'href="https://([^"]+)"', t)
                            if m: host = m.group(1)
                except: pass

                # 2. æœç´¢è¯·æ±‚ - ä½¿ç”¨æµè§ˆå™¨é£æ ¼çš„ GET è¯·æ±‚
                search_url = f"https://{host}/search.php?mod=forum&searchsubmit=yes&kw={keyword}"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                    'Cookie': cookie,
                    'Referer': f"https://{host}/search.php?mod=forum"
                }
                
                logger.info(f"[sxsy æœç´¢] å°è¯• GET æœç´¢: {search_url}")
                async with session.get(search_url, headers=headers, timeout=15, ssl=False) as response:
                    html = await self._get_text(response)
                    logger.info(f"[sxsy æœç´¢] å“åº” URL: {response.url}, é•¿åº¦: {len(html)}")

                    # å¦‚æœç›´æ¥ GET æ²¡ç»“æœï¼Œä¸”æ²¡æœ‰ searchidï¼Œè¯´æ˜å¯èƒ½éœ€è¦ POST
                    if ("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æœç´¢ç»“æœ" in html or "threadlist" not in html) and "searchid=" not in str(response.url):
                        logger.info("[sxsy æœç´¢] ç›´æ¥ GET æœªèƒ½è·å–ç»“æœï¼Œå°è¯• POST æ–¹æ¡ˆ...")
                        post_url = f"https://{host}/search.php?mod=forum"
                        # å°è¯•è·å– formhash
                        formhash = ""
                        try:
                            async with session.get(post_url, headers=headers, timeout=10, ssl=False) as f_resp:
                                f_html = await self._get_text(f_resp)
                                fh_match = re.search(r'name="formhash" value="([a-f0-9]+)"', f_html)
                                if fh_match: formhash = fh_match.group(1)
                        except: pass

                        post_data = {
                            'mod': 'forum',
                            'searchsubmit': 'yes',
                            'srchtxt': keyword,
                            'formhash': formhash
                        }
                        async with session.post(post_url, data=post_data, headers=headers, timeout=15, ssl=False) as p_resp:
                            html = await self._get_text(p_resp)
                            logger.info(f"[sxsy æœç´¢] POST å“åº” URL: {p_resp.url}, é•¿åº¦: {len(html)}")

                    if "è¯·å…ˆç™»å½•" in html or "è®¿é—®é™åˆ¶" in html:
                        yield event.plain_result("âŒ Cookie å¯èƒ½å·²å¤±æ•ˆï¼Œè¯·é‡æ–°é…ç½®ã€‚")
                        return

                    if "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æœç´¢ç»“æœ" in html:
                        yield event.plain_result(f"ğŸ“¦ æœªæ‰¾åˆ°ä¸ â€œ{keyword}â€ ç›¸å…³çš„æœç´¢ç»“æœã€‚")
                        return

                    # 3. è§£æç»“æœ
                    soup = BeautifulSoup(html, 'lxml')
                    items = soup.select('div#threadlist ul li.pbw') or soup.select('div.slst ul li.pbw')
                    logger.info(f"[sxsy æœç´¢] è§£æåˆ° {len(items)} æ¡ç»“æœ")

                    if not items:
                        logger.error(f"[sxsy æœç´¢] æ— æ³•è§£æåˆ° li.pbwã€‚HTML ç‰‡æ®µ:\n{html[:1000]}")
                        yield event.plain_result("âŒ æ— æ³•è§£ææœç´¢ç»“æœï¼Œè¯·æ£€æŸ¥ Cookie æˆ–ç½‘ç«™ç»“æ„æ˜¯å¦å˜åŒ–ã€‚")
                        return

                    results = []
                    for item in items[:5]:
                        # æ ‡é¢˜å’Œé“¾æ¥
                        title_el = item.select_one('h3.xs3 a')
                        if not title_el: continue
                        
                        # æ¸…ç†æ ‡é¢˜ä¸­çš„ HTML æ ‡ç­¾
                        title = "".join(title_el.find_all(string=True, recursive=True)).strip()
                        link = urljoin(f"https://{host}/", title_el['href'])
                        
                        # å‘å¸–æ—¶é—´
                        time_text = ""
                        ps = item.find_all('p', recursive=False)
                        if ps:
                            last_p = ps[-1]
                            time_span = last_p.select_one('span')
                            if time_span:
                                time_text = time_span.get_text(strip=True)
                        
                        results.append(f"ğŸ“Œ {title}\nğŸ”— {link}\nğŸ“… æ—¶é—´: {time_text}")

                    if not results:
                        yield event.plain_result("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„æœç´¢ç»“æœã€‚")
                    else:
                        reply = f"âœ… ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹å…³äº â€œ{keyword}â€ çš„ç»“æœï¼š\n\n" + "\n\n".join(results)
                        yield event.plain_result(reply)

            except Exception as e:
                logger.error(f"sxsy æœç´¢å‡ºé”™: {e}")
                yield event.plain_result(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    @filter.command("sis", alias={'ç¬¬ä¸€ä¼šæ‰€'})
    async def sis_command(self, event: AstrMessageEvent):
        """
        è·å–ç¬¬ä¸€ä¼šæ‰€çš„ç½‘å€ã€‚
        ç”¨æ³•: /sis
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /sis å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾ç¬¬ä¸€ä¼šæ‰€ç½‘å€ã€‚")
        
        target_navs = ["http://sis001dz.org/", "http://www.sis001home.com/"]
        
        async with aiohttp.ClientSession() as session:
            for url in target_navs:
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                    }
                    async with session.get(url, headers=headers, timeout=10) as response:
                        if response.status == 200:
                            text = await self._get_text(response)
                            soup = BeautifulSoup(text, 'lxml')
                            link_element = soup.find('a', string=re.compile(r'åœ°å€ä¸€'))
                            if link_element and link_element.has_attr('href'):
                                link_url = link_element['href']
                                yield event.plain_result(f"ğŸ” æˆåŠŸæ‰¾åˆ°ç¬¬ä¸€ä¼šæ‰€æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                                return
                except Exception as e:
                    logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
                    continue
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œç¬¬ä¸€ä¼šæ‰€å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    @filter.command("01bz", alias={'ç¬¬ä¸€ç‰ˆä¸»'})
    async def dybz_command(self, event: AstrMessageEvent):
        """
        è·å–ç¬¬ä¸€ç‰ˆä¸»çš„ç½‘å€ã€‚
        ç”¨æ³•: /01bz
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /01bz å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾ç¬¬ä¸€ç‰ˆä¸»ç½‘å€ã€‚")
        
        target_navs = ["https://www.é¾™è…¾å°è¯´.com/", "http://01bz.cc/"]
        
        async with aiohttp.ClientSession() as session:
            for url in target_navs:
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                    }
                    async with session.get(url, headers=headers, timeout=10) as response:
                        if response.status == 200:
                            text = await self._get_text(response)
                            soup = BeautifulSoup(text, 'lxml')
                            link_element = soup.find('a', string=re.compile(r'æœ€æ–°çº¿è·¯\s*1'))
                            if link_element and link_element.has_attr('href'):
                                link_url = link_element['href']
                                yield event.plain_result(f"ğŸ“š æˆåŠŸæ‰¾åˆ°ç¬¬ä¸€ç‰ˆä¸»æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                                return
                except Exception as e:
                    logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
                    continue
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œç¬¬ä¸€ç‰ˆä¸»å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    @filter.command("uaa", alias={'æœ‰çˆ±çˆ±'})
    async def uaa_command(self, event: AstrMessageEvent):
        """
        è·å–æœ‰çˆ±çˆ±çš„ç½‘å€ã€‚
        ç”¨æ³•: /uaa
        """
        logger.info(f"ç”¨æˆ· {event.get_sender_name()} è§¦å‘ /uaa å‘½ä»¤ï¼Œå¼€å§‹æŸ¥æ‰¾æœ‰çˆ±çˆ±ç½‘å€ã€‚")
        
        url = "https://uaadizhi.com/"
        
        async with aiohttp.ClientSession() as session:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0',
                }
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        text = await self._get_text(response)
                        soup = BeautifulSoup(text, 'lxml')
                        li_elements = soup.find_all('li')
                        for li in li_elements:
                            span = li.find('span')
                            if span and 'æœ€æ–°' in span.get_text():
                                a_tag = li.find('a')
                                if a_tag and a_tag.has_attr('href'):
                                    link_url = a_tag['href']
                                    yield event.plain_result(f"ğŸ’• æˆåŠŸæ‰¾åˆ°æœ‰çˆ±çˆ±æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                                    return
            except Exception as e:
                logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
            
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæœ‰çˆ±çˆ±å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    async def terminate(self):
        """æ’ä»¶é”€æ¯æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("æœä¹¦å§é“¾æ¥è·å–æ’ä»¶å·²å¸è½½")