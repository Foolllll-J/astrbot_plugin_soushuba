import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
import os
import re

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api.message_components import Plain
from astrbot.api import logger

@register(
    "astrbot_plugin_soushuba",
    "Foolllll",
    "æœä¹¦å§åŠ©æ‰‹",
    "1.1.0",
    "https://github.com/Foolllll-J/astrbot_plugin_soushuba",
)
class SoushuBaLinkExtractorPlugin(Star):
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        self.target_domains: List[str] = [
            "https://soushu2022.com",
            "https://soushu2025.com",
            "https://soushu2030.com",
            "https://soushu2035.com",
        ]
        self.plugin_config = config
        self.search_result_count = config.get("search_result_count", 10)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

    async def _get_text(self, response: aiohttp.ClientResponse) -> str:
        """è·å–å“åº”å†…å®¹å¹¶å¤„ç†ç¼–ç é—®é¢˜"""
        content = await response.read()
        
        charset = response.charset
        if charset:
            try:
                return content.decode(charset)
            except:
                pass
        
        for encoding in ['utf-8', 'gbk', 'gb2312', 'big5']:
            try:
                return content.decode(encoding)
            except UnicodeDecodeError:
                continue
        
        return content.decode('utf-8', errors='ignore')

    async def _extract_link_from_url(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """å°è¯•è®¿é—®URLå¹¶æå–æŒ‡å®šé“¾æ¥ã€‚æˆåŠŸåˆ™è¿”å›é“¾æ¥ï¼Œå¤±è´¥è¿”å› Noneã€‚"""
        try:
            ssl_verify = False if url.startswith("https://") else True
            
            async with session.get(url, timeout=20, ssl=ssl_verify) as response:
                final_url = str(response.url)
                html_content = await self._get_text(response)
            
            js_redirect_match = re.search(r"window\.location\.href\s*=\s*['\"](.*?)['\"];", html_content)
            if js_redirect_match:
                redirect_target_url = urljoin(final_url, js_redirect_match.group(1))
                return await self._extract_link_from_url(session, redirect_target_url)

            meta_refresh_match = re.search(r"<meta http-equiv=\"refresh\" content=\"[\d\.]*;\s*url=(.*?)\"", html_content, re.IGNORECASE)
            if meta_refresh_match:
                redirect_target_url = urljoin(final_url, meta_refresh_match.group(1))
                return await self._extract_link_from_url(session, redirect_target_url)

            soup = BeautifulSoup(html_content, 'lxml') 
            link_element = soup.select_one('a.link') 
            if not link_element:
                link_element = soup.find('a', string='æœä¹¦å§')
            if not link_element:
                link_element = soup.find('a')

            if link_element and link_element.has_attr('href'):
                link_url = link_element['href']
                if not link_url.startswith(('http://', 'https://')):
                    link_url = urljoin(final_url, link_url)
                return link_url

        except Exception as e: 
            logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
        return None

    @filter.command("ssb", alias={'æœä¹¦å§'})
    async def ssb_command(self, event: AstrMessageEvent):
        """è·å–æœä¹¦å§çš„ç½‘å€"""
        async with aiohttp.ClientSession() as session:
            for domain_url in self.target_domains:
                link_url = await self._extract_link_from_url(session, domain_url)
                if link_url:
                    yield event.plain_result(f"ğŸ“– æˆåŠŸæ‰¾åˆ°æœä¹¦å§æœ€æ–°ç½‘å€ï¼š\n{link_url}")
                    return
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæ‰€æœ‰å¯¼èˆªç½‘ç«™å‡æ— æ³•è®¿é—®æˆ–æœªæ‰¾åˆ°å¯ç”¨é“¾æ¥ã€‚")

    @filter.command("sxsy", alias={'å°šé¦™ä¹¦è‹‘'})
    async def sxsy_command(self, event: AstrMessageEvent):
        """å°šé¦™ä¹¦è‹‘æœç´¢"""
        args = event.message_str.strip().split(maxsplit=1)
        if len(args) < 2:
            # åŸºç¡€ç½‘å€è·å–é€»è¾‘
            async with aiohttp.ClientSession() as session:
                try:
                    url = "https://sxsy.org/"
                    async with session.get(url, headers=self.headers, timeout=10, ssl=False) as response:
                        if response.status == 200:
                            text = await self._get_text(response)
                            match = re.search(r'href="https://([^"]+)"', text)
                            if match:
                                yield event.plain_result(f"ğŸŒ¸ æˆåŠŸæ‰¾åˆ°å°šé¦™ä¹¦è‹‘æœ€æ–°ç½‘å€ï¼š\nhttps://{match.group(1)}")
                                return
                except Exception as e:
                    logger.error(f"[è·å–sxsy host] é”™è¯¯: {e}")
            yield event.plain_result("âŒ æŠ±æ­‰ï¼Œå°šé¦™ä¹¦è‹‘å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")
            return

        keyword = args[1]
        cookie = self.plugin_config.get("sxsy_cookie", "") if self.plugin_config else ""
        if not cookie:
            yield event.plain_result("âŒ è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­è®¾ç½® sxsy_cookieã€‚")
            return

        yield event.plain_result(f"ğŸ” æ­£åœ¨å°šé¦™ä¹¦è‹‘æœç´¢: {keyword}...")

        async with aiohttp.ClientSession() as session:
            try:
                # 1. è·å–æœ€æ–° host
                host = "sxsy87.com"
                try:
                    async with session.get("https://sxsy.org/", timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            t = await self._get_text(resp)
                            m = re.search(r'href="https://([^"]+)"', t)
                            if m: host = m.group(1)
                except: pass

                # 2. å‡†å¤‡ POST è¯·æ±‚
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
                    'Cookie': cookie,
                    'Referer': f"https://{host}/search.php?mod=forum"
                }
                post_url = f"https://{host}/search.php?mod=forum"

                # æå– formhash
                formhash = ""
                try:
                    async with session.get(post_url, headers=headers, timeout=10, ssl=False) as f_resp:
                        f_html = await self._get_text(f_resp)
                        fh_match = re.search(r'name="formhash" value="([a-f0-9]+)"', f_html)
                        if fh_match: formhash = fh_match.group(1)
                except: pass

                post_data = {
                    'mod': 'forum',
                    'searchsubmit': 'yes',
                    'srchtxt': keyword,
                    'formhash': formhash
                }

                # 3. å‘é€ POST æœç´¢
                logger.info(f"[sxsy æœç´¢] å°è¯• POST æœç´¢: {post_url}")
                async with session.post(post_url, data=post_data, headers=headers, timeout=15, ssl=False) as p_resp:
                    html = await self._get_text(p_resp)
                    logger.info(f"[sxsy æœç´¢] POST å“åº” URL: {p_resp.url}, é•¿åº¦: {len(html)}")

                # 4. æ£€æŸ¥å¼‚å¸¸çŠ¶æ€
                if "è¯·å…ˆç™»å½•" in html or "è®¿é—®é™åˆ¶" in html:
                    yield event.plain_result("âŒ Cookie å¯èƒ½å·²å¤±æ•ˆï¼Œè¯·é‡æ–°é…ç½®ã€‚")
                    return
                if "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æœç´¢ç»“æœ" in html:
                    yield event.plain_result(f"ğŸ“¦ æœªæ‰¾åˆ°ä¸ â€œ{keyword}â€ ç›¸å…³çš„æœç´¢ç»“æœã€‚")
                    return

                # 5. è§£æç»“æœ
                soup = BeautifulSoup(html, 'lxml')
                items = soup.select('div#threadlist ul li.pbw') or soup.select('div.slst ul li.pbw')
                logger.info(f"[sxsy æœç´¢] è§£æåˆ° {len(items)} æ¡ç»“æœ")

                if not items:
                    yield event.plain_result("âŒ æ— æ³•è·å–æœç´¢ç»“æœï¼Œè¯·æ£€æŸ¥ Cookie æ˜¯å¦è¿‡æœŸã€‚")
                    return

                results = []
                for item in items[:self.search_result_count]:
                    title_el = item.select_one('h3.xs3 a')
                    if not title_el: continue
                    
                    title = "".join(title_el.find_all(string=True, recursive=True)).strip()
                    link = urljoin(f"https://{host}/", title_el['href'])
                    
                    # æå–æ—¶é—´
                    time_text = "æœªçŸ¥"
                    time_span = item.select_one('p span') # Discuz æœç´¢é¡µé€šå¸¸ç¬¬ä¸€ä¸ª span æ˜¯æ—¶é—´
                    if time_span:
                        time_text = time_span.get_text(strip=True)
                    
                    results.append(f"ğŸ“Œ {title}\nğŸ”— {link}\nğŸ“… æ—¶é—´: {time_text}")

                reply = f"âœ… ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹å…³äº â€œ{keyword}â€ çš„ç»“æœï¼š\n\n" + "\n\n".join(results)
                yield event.plain_result(reply)

            except Exception as e:
                logger.error(f"sxsy æœç´¢å‡ºé”™: {e}")
                yield event.plain_result(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ï¼Œè¯·ç¨åé‡è¯•ã€‚")

    @filter.command("sis", alias={'ç¬¬ä¸€ä¼šæ‰€'})
    async def sis_command(self, event: AstrMessageEvent):
        """è·å–ç¬¬ä¸€ä¼šæ‰€çš„ç½‘å€"""
        target_navs = ["http://sis001dz.org/", "http://www.sis001home.com/"]
        async with aiohttp.ClientSession() as session:
            for url in target_navs:
                try:
                    async with session.get(url, headers=self.headers, timeout=10) as response:
                        if response.status == 200:
                            text = await self._get_text(response)
                            soup = BeautifulSoup(text, 'lxml')
                            link_element = soup.find('a', string=re.compile(r'åœ°å€ä¸€'))
                            if link_element and link_element.has_attr('href'):
                                yield event.plain_result(f"ğŸ” æˆåŠŸæ‰¾åˆ°ç¬¬ä¸€ä¼šæ‰€æœ€æ–°ç½‘å€ï¼š\n{link_element['href']}")
                                return
                except: continue
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œç¬¬ä¸€ä¼šæ‰€å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    @filter.command("01bz", alias={'ç¬¬ä¸€ç‰ˆä¸»'})
    async def dybz_command(self, event: AstrMessageEvent):
        """è·å–ç¬¬ä¸€ç‰ˆä¸»çš„ç½‘å€"""
        target_navs = ["https://www.é¾™è…¾å°è¯´.com/", "http://01bz.cc/"]
        async with aiohttp.ClientSession() as session:
            for url in target_navs:
                try:
                    async with session.get(url, headers=self.headers, timeout=10) as response:
                        if response.status == 200:
                            text = await self._get_text(response)
                            soup = BeautifulSoup(text, 'lxml')
                            link_element = soup.find('a', string=re.compile(r'æœ€æ–°çº¿è·¯\s*1'))
                            if link_element and link_element.has_attr('href'):
                                yield event.plain_result(f"ğŸ“š æˆåŠŸæ‰¾åˆ°ç¬¬ä¸€ç‰ˆä¸»æœ€æ–°ç½‘å€ï¼š\n{link_element['href']}")
                                return
                except: continue
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œç¬¬ä¸€ç‰ˆä¸»å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    @filter.command("uaa", alias={'æœ‰çˆ±çˆ±'})
    async def uaa_command(self, event: AstrMessageEvent):
        """è·å–æœ‰çˆ±çˆ±çš„ç½‘å€"""
        url = "https://uaadizhi.com/"
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, headers=self.headers, timeout=10) as response:
                    if response.status == 200:
                        text = await self._get_text(response)
                        soup = BeautifulSoup(text, 'lxml')
                        for li in soup.find_all('li'):
                            span = li.find('span')
                            if span and 'æœ€æ–°' in span.get_text():
                                a_tag = li.find('a')
                                if a_tag:
                                    yield event.plain_result(f"ğŸ’• æˆåŠŸæ‰¾åˆ°æœ‰çˆ±çˆ±æœ€æ–°ç½‘å€ï¼š\n{a_tag['href']}")
                                    return
            except: pass
        yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæœ‰çˆ±çˆ±å¯¼èˆªç«™ç›®å‰æ— æ³•è®¿é—®ã€‚")

    async def terminate(self):
        logger.info("æœä¹¦å§é“¾æ¥è·å–æ’ä»¶å·²å¸è½½")